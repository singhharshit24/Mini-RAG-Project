ğŸ§  Mini RAG Project â€“ Local Retrieval-Augmented Generation Assistant

A lightweight, fully local Retrieval-Augmented Generation (RAG) system built in Python using:

FAISS (vector similarity search)

SentenceTransformers (embeddings)

Ollama + LLaMA3 (local LLM inference)

This project demonstrates how to build a document-grounded AI assistant that answers questions strictly based on uploaded documents.

ğŸš€ Features

âœ… Multi-document support

âœ… Local embeddings

âœ… FAISS vector database

âœ… Local LLM (LLaMA3 via Ollama)

âœ… Interactive CLI assistant

âœ… Source tracking for retrieved chunks

âœ… No cloud dependency

ğŸ— Architecture
                +------------------+
                |  Text Documents  |
                +------------------+
                          |
                          v
                +------------------+
                |   Chunking       |
                +------------------+
                          |
                          v
                +------------------+
                |  Embeddings      |
                | (MiniLM Model)   |
                +------------------+
                          |
                          v
                +------------------+
                |   FAISS Index    |
                +------------------+
                          |
User Query  --->   Embedding ---> Vector Search
                          |
                          v
                +------------------+
                |  Retrieved       |
                |   Context        |
                +------------------+
                          |
                          v
                +------------------+
                |   LLaMA3 (LLM)   |
                |  via Ollama      |
                +------------------+
                          |
                          v
                      Final Answer

ğŸ›  Tech Stack

Python 3.8+

FAISS (vector search)

SentenceTransformers

Ollama

LLaMA3 (local model)

PyTorch

ğŸ“‚ Project Structure
Mini-RAG-Project/
â”‚
â”œâ”€â”€ documents/
â”‚   â””â”€â”€ sample.txt
â”‚
â”œâ”€â”€ rag.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

âš™ï¸ Installation
1ï¸âƒ£ Clone Repository
git clone https://github.com/singhharshit24/Mini-RAG-Project.git
cd Mini-RAG-Project

2ï¸âƒ£ Create Virtual Environment
python -m venv rag_env
rag_env\Scripts\activate   # Windows
# source rag_env/bin/activate   # Mac/Linux

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Install Ollama

Download from:
https://ollama.com

Pull LLaMA3 model:

ollama pull llama3

â–¶ï¸ How to Run

Make sure:

documents/ folder contains .txt files

Virtual environment is activated

Run:

python rag.py


You will see:

RAG Assistant Ready. Type 'exit' to quit.


Ask questions interactively.

Type exit to close.

ğŸ’¬ Example Usage

If your document contains information about an AI summit:

You: When is the AI summit happening?
Assistant: The AI Impact Summit India 2026 is scheduled from 16â€“21 February 2026.


If information is not in the documents:

Assistant: I don't know based on the provided documents.

ğŸ§  What is RAG?

Retrieval-Augmented Generation (RAG) is an architecture that:

Retrieves relevant documents using embeddings

Injects them into the LLM prompt

Generates grounded responses

The LLM weights are NOT trained or modified.

Knowledge expansion happens through vector database indexing.

ğŸ” How It Works Internally

Documents are split into chunks

Chunks are converted into embeddings

Embeddings are stored in FAISS

User query is embedded

Top-k similar chunks are retrieved

Context + query sent to LLaMA3

Answer generated

ğŸ“ˆ Possible Improvements

Persistent FAISS index storage

Better chunking with token overlap

Hybrid search (BM25 + embeddings)

Reranker model

Web UI (FastAPI / Streamlit)

Conversation memory

Agent-based tool usage

ğŸ¯ Learning Outcomes

This project demonstrates:

Vector similarity search

Embedding-based retrieval

Prompt grounding

Local LLM deployment

Building scalable AI pipelines