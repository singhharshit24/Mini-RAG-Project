# ğŸ§  Mini RAG â€” Local Retrieval-Augmented Generation Assistant

A lightweight, fully local RAG system that answers questions grounded strictly in your documents â€” no cloud, no API keys, no data leaving your machine.

---

## âœ¨ Features

- **Multi-document support** â€” drop in as many `.txt` files as you need
- **Local embeddings** â€” powered by SentenceTransformers (MiniLM)
- **FAISS vector search** â€” fast similarity retrieval over your document chunks
- **Local LLM** â€” LLaMA3 via Ollama runs entirely on your hardware
- **Source tracking** â€” see exactly which chunks informed each answer
- **Interactive CLI** â€” simple question-answer loop, no UI overhead

---

## ğŸ— Architecture

```
Text Documents
      â”‚
      â–¼
  Chunking
      â”‚
      â–¼
 Embeddings (MiniLM)
      â”‚
      â–¼
  FAISS Index â—„â”€â”€â”€â”€ User Query â”€â”€â–º Query Embedding
      â”‚                                    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Top-K Chunks â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
              LLaMA3 via Ollama
                        â”‚
                        â–¼
                  Final Answer
```

---

## ğŸ›  Tech Stack

| Component | Tool |
|---|---|
| Language | Python 3.8+ |
| Embeddings | SentenceTransformers (MiniLM) |
| Vector Search | FAISS |
| LLM | LLaMA3 |
| LLM Runtime | Ollama |
| ML Backend | PyTorch |

---

## ğŸ“‚ Project Structure

```
Mini-RAG-Project/
â”‚
â”œâ”€â”€ documents/
â”‚   â””â”€â”€ sample.txt        # Add your .txt files here
â”‚
â”œâ”€â”€ rag.py                # Main application
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1. Clone the repository
```bash
git clone https://github.com/singhharshit24/Mini-RAG-Project.git
cd Mini-RAG-Project
```

### 2. Create and activate a virtual environment
```bash
python -m venv rag_env

# Windows
rag_env\Scripts\activate

# macOS / Linux
source rag_env/bin/activate
```

### 3. Install Python dependencies
```bash
pip install -r requirements.txt
```

### 4. Install Ollama and pull LLaMA3

Download Ollama from [ollama.com](https://ollama.com), then run:
```bash
ollama pull llama3
```

---

## â–¶ï¸ Running the Assistant

Make sure your `documents/` folder contains at least one `.txt` file and your virtual environment is active, then:

```bash
python rag.py
```

You'll see:
```
RAG Assistant Ready. Type 'exit' to quit.

You:
```

Type your question and press Enter. Type `exit` to quit.

---

## ğŸ’¬ Example

**Document contains:** information about an AI summit in India

```
You: When is the AI summit happening?
Assistant: The AI Impact Summit India 2026 is scheduled from 16â€“21 February 2026.

You: Who is the keynote speaker?
Assistant: I don't know based on the provided documents.
```

The assistant will never hallucinate outside your documents â€” if the answer isn't there, it says so.

---

## ğŸ” How It Works

1. Documents in `documents/` are split into overlapping text chunks
2. Each chunk is converted to a vector embedding using MiniLM
3. Embeddings are indexed in FAISS for fast similarity search
4. When you ask a question, it's embedded using the same model
5. The top-K most similar chunks are retrieved from FAISS
6. Those chunks + your question are sent to LLaMA3 as context
7. LLaMA3 generates an answer grounded in the retrieved context

The LLM's weights are never modified â€” all knowledge expansion happens through the vector index.

---

## ğŸš€ Possible Improvements

- Persistent FAISS index (skip re-indexing on restart)
- Overlapping chunking strategy for better context continuity
- Hybrid search (BM25 + dense embeddings)
- Reranker model for improved chunk selection
- Web UI with FastAPI or Streamlit
- Conversation memory across turns
- Agent-based tool usage

---

## ğŸ¯ What You'll Learn from This Project

- How vector similarity search works in practice
- Embedding-based document retrieval
- Prompt grounding to reduce hallucination
- Local LLM deployment with Ollama
- The fundamentals of building a production-ready AI pipeline
